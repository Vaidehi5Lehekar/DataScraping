# -*- coding: utf-8 -*-
"""Data Scraping and analysis for Blackcoffer

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13wqMrvWaBgfgVJ3kYC_iGRFomJdAUXu-
"""

!pip install beautifulsoup4
!pip install requests
!pip install urllib2

import nltk
nltk.download('punkt')
from textblob import TextBlob

##TEXTUAL ANALYSIS

##CLEANING USING STOP WORDS

stop_words = []
stop_words_directory = "/content/drive/MyDrive/DataScraper/StopWords"  # Replace with the actual path to the "StopWords" folder

for filename in os.listdir(stop_words_directory):
    if filename.endswith(".txt"):
        with open(os.path.join(stop_words_directory, filename), 'r', encoding='latin-1') as file:
            stop_words.extend(file.read().split())

import nltk
from nltk.corpus import stopwords

stop_words_directory ='/content/drive/MyDrive/DataScraper/StopWords'
# Function to tokenize and clean text
def clean_text(text, words):
  words = nltk.word_tokenize(text)
  cleaned_words = [word for word in words if word.lower() not in stop_words]
  cleaned_text = ' '.join(cleaned_words)
  return cleaned_text

  '''
  # Custom stopwords from individual files in the directory
  custom_stopwords = set()
  for filename in os.listdir(stop_words_directory):
    if filename.endswith(".txt"):
      with open(os.path.join(stop_words_directory, filename), "r") as file:
        custom_stopwords.update(set(line.strip() for line in file))
  # Combine custom stopwords with NLTK stopwords
  all_stopwords = stop_words.union(custom_stopwords)
'''

import requests
from bs4 import BeautifulSoup
import pandas as pd
import os
import urllib3


# Function to scrape a URL and extract data
def scrape_url(url):
    response = requests.get(url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, "html.parser")

        # Example: Extract the article text
        title = soup.title.get_text() if soup.title else "No Title"
        paragraphs = soup.find_all('p') #, attrs ={'speechify-initial-font-family':"Verdana, &quot;system-ui&quot;, -apple-system, &quot;Segoe UI&quot;, Roboto, Oxygen, Ubuntu, Cantarell, &quot;Open Sans&quot;, &quot;Helvetica Neue&quot;, sans-serif"})
        article_text = ' '.join([p.get_text() for p in paragraphs])
        article =title+"     " +article_text
        return article
    else:
        print("Failed to retrieve the webpage. Status code:", response.status_code)
        return None

# Read URLs from the second Excel file
urls_df = pd.read_excel("/content/drive/MyDrive/DataScraper/Input.xlsx")  # Replace with your file path

# Specify the directory where you want to save the text files
save_directory = "/content/drive/MyDrive/DataScraper"  # Replace with the desired directory path


# Create the directory if it doesn't exist
os.makedirs(save_directory, exist_ok=True)

# Iterate over the rows and scrape each URL
for index, row in urls_df.iterrows():
   url_id = row['URL_ID']
   url = row["URL"]  # Replace with the actual column name containing URLs
   article = scrape_url(url)
   article_text_str = str(article)
   # Specify the full path for the text file
   file_path = os.path.join(save_directory, f"{url_id}.txt")
   #cleaned_article_text = clean_text(article_text_str, stop_words)
   # Write the article text to the specified text file
   with open(file_path, "w", encoding="utf-8") as text_file:
    article_cleaned = clean_text(article_text_str, stop_words_directory)
    text_file.write(article_cleaned)

from textblob import TextBlob

def calculate_variables(text):
    analysis = TextBlob(text)
    sentence_list = nltk.sent_tokenize(text)
    word_tokens = nltk.word_tokenize(text)
    word_count = len(word_tokens)
    complex_words = [word for word in word_tokens if len(word) > 6]
    complex_word_count = len(complex_words)
    syllable_per_word = sum(len(word) for word in word_tokens)
    personal_pronouns = text.count('I') + text.count('me') + text.count('my') + text.count('mine')
    avg_word_length = syllable_per_word / word_count if word_count > 0 else 0
    avg_sentence_length = word_count / len(sentence_list) if len(sentence_list) > 0 else 0
    percentage_complex_words = (complex_word_count / word_count) * 100 if word_count > 0 else 0
    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)
    return analysis.sentiment.polarity, -analysis.sentiment.polarity, analysis.sentiment.polarity, analysis.sentiment.subjectivity, avg_sentence_length, percentage_complex_words, fog_index, avg_sentence_length, complex_word_count, word_count, syllable_per_word, personal_pronouns, avg_word_length

output_file = "/content/drive/MyDrive/DataScraper/Output Data Structure.xlsx"  # Replace with the actual path to your output file
output_data = pd.read_excel(output_file)

for index, row in urls_df.iterrows():
  for index, row in output_data.iterrows():
    variables = calculate_variables(article_cleaned)
    output_data.at[index, 'POSITIVE SCORE'] = variables[0]
    output_data.at[index, 'NEGATIVE SCORE'] = variables[1]
    output_data.at[index, 'POLARITY SCORE'] = variables[2]
    output_data.at[index, 'SUBJECTIVITY SCORE'] = variables[3]
    output_data.at[index, 'AVG SENTENCE LENGTH'] = variables[4]
    output_data.at[index, 'PERCENTAGE OF COMPLEX WORDS'] = variables[5]
    output_data.at[index, 'FOG INDEX'] = variables[6]
    output_data.at[index, 'AVG NUMBER OF WORDS PER SENTENCE'] = variables[7]
    output_data.at[index, 'COMPLEX WORD COUNT'] = variables[8]
    output_data.at[index, 'WORD COUNT'] = variables[9]
    output_data.at[index, 'SYLLABLE PER WORD'] = variables[10]
    output_data.at[index, 'PERSONAL PRONOUNS'] = variables[11]
    output_data.at[index, 'AVG WORD LENGTH'] = variables[12]


# Save the updated DataFrame with calculated variables to the output file
output_data.to_excel(output_file, index=False)



